{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0d308d",
   "metadata": {},
   "source": [
    "## One Hot Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ad0470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Leapfrog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Leapfrog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\Leapfrog\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc72c8",
   "metadata": {},
   "source": [
    "### We take only 1 sentence as input here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4c0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"We are reading about Natural Language Processing Here\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8a8118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    We are reading about Natural Language Processi...\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.Series(sentence)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8420d8",
   "metadata": {},
   "source": [
    "## Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dce0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(corpus, keep_list):\n",
    "    '''\n",
    "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
    "    \n",
    "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
    "            even after the cleaning process\n",
    "    \n",
    "    Output : Returns the cleaned text corpus\n",
    "    \n",
    "    '''\n",
    "    cleaned_corpus = pd.Series()\n",
    "    for row in corpus:\n",
    "        qs = []\n",
    "        for word in row.split():\n",
    "            if word not in keep_list:\n",
    "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
    "                p1 = p1.lower()\n",
    "                qs.append(p1)\n",
    "            else : qs.append(word)\n",
    "        cleaned_corpus = cleaned_corpus._append(pd.Series(' '.join(qs)))\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "477cf707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(corpus):\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f27fa3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(corpus, stem_type = None):\n",
    "    if stem_type == 'snowball':\n",
    "        stemmer = SnowballStemmer(language = 'english')\n",
    "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    else :\n",
    "        stemmer = PorterStemmer()\n",
    "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "583c8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_removal(corpus):\n",
    "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for word in wh_words:\n",
    "        stop.remove(word)\n",
    "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a074b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
    "    '''\n",
    "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
    "    \n",
    "    Input : \n",
    "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
    "    'keep_list' - List of words to be retained during cleaning process\n",
    "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n",
    "                                                                  be performed or not\n",
    "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
    "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
    "    \n",
    "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
    "    \n",
    "    Output : Returns the processed text corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if cleaning == True:\n",
    "        corpus = text_clean(corpus, keep_list)\n",
    "    \n",
    "    if remove_stopwords == True:\n",
    "        corpus = stopwords_removal(corpus)\n",
    "    else :\n",
    "        corpus = [[x for x in x.split()] for x in corpus]\n",
    "    \n",
    "    if lemmatization == True:\n",
    "        corpus = lemmatize(corpus)\n",
    "        \n",
    "        \n",
    "    if stemming == True:\n",
    "        corpus = stem(corpus, stem_type)\n",
    "    \n",
    "    corpus = [' '.join(x) for x in corpus]        \n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c110d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read natural language process']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing with Lemmatization here\n",
    "preprocessed_corpus = preprocess(corpus, keep_list = [], stemming = False, stem_type = None,\n",
    "                                lemmatization = True, remove_stopwords = True)\n",
    "preprocessed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c21cbd",
   "metadata": {},
   "source": [
    "## Building the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f7f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['process', 'natural', 'read', 'language']\n"
     ]
    }
   ],
   "source": [
    "set_of_words = set()\n",
    "for word in preprocessed_corpus[0].split():\n",
    "    set_of_words.add(word)\n",
    "vocab = list(set_of_words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2e325",
   "metadata": {},
   "source": [
    "## Fetching the position of each word in the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a912a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'process': 0, 'natural': 1, 'read': 2, 'language': 3}\n"
     ]
    }
   ],
   "source": [
    "position = {}\n",
    "for i, token in enumerate(vocab):\n",
    "    position[token] = i\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3766dce",
   "metadata": {},
   "source": [
    "## Instantiating the one hot matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0dea85",
   "metadata": {},
   "source": [
    "### Note here every row in the matrix corresponds to the One Hot vector for an individual term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e8e144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_matrix = np.zeros((len(preprocessed_corpus[0].split()), len(vocab)))\n",
    "one_hot_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cad0d7",
   "metadata": {},
   "source": [
    "## Building One Hot Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1363e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(preprocessed_corpus[0].split()):\n",
    "    one_hot_matrix[i][position[token]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0936d0",
   "metadata": {},
   "source": [
    "## Visualizing the One Hot Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f668382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cd49e",
   "metadata": {},
   "source": [
    "## Inference\n",
    "The first row corresponds to the One Hot vector of read,\n",
    "\n",
    "second for natural,\n",
    "\n",
    "third for language and\n",
    "\n",
    "the final one for process\n",
    "\n",
    "based on their respective indices in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308daf38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
